\documentclass{article}
\usepackage{amsmath, graphicx, amsfonts, caption, subfig, keyval, algorithm, algorithmic,xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}

\captionsetup{justification=centering}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\X}{\mathcal{X}}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\onehalfspacing

\begin{document}
\title{AM221 Final Project Proposal \\ \large Dictionary Selection Under a Supermodular Assumption}
\author{Taylor Killian \& Leonhard Spiegelberg}
\maketitle

%\begin{abstract}
%To be filled with an interesting summary of our work and results
%
%\end{abstract}

\section{Collaboration}\label{partnership}
As noted above, this project is to be completed by Taylor Killian and Leonhard Spiegelberg with careful guidance by Prof. Singer. We anticipate that this project will take our full coordinated effort to complete a worthwhile result by the end of the semester. We have already begun trying to understand the literature and underlying structure of the Sparse Regression/Dictionary Selection problem formulation, including all assumptions that have been made to facilitate approximate solutions. We anticipate that multiple meetings per week will be necessary to ensure consistent progress. 

\section{Model/Problem} \label{model}
Sparse Regression / Dictionary Selection is a class of problems that are variants of representation learning. The goal of these kinds of problems is to determine a sparse representation of input data in the form of a linear combination of basis elements as well as the basis elements themselves. That is, one essentially factors the input matrix in a sparse catalog and a dictionary of basis elements both of which need to be inferred from the data.
\newline

\noindent More formally this problem has been defined as follows:
\newline

Given data $\mathcal{X} = [x_1,\dots , x_k], \ x_i\in\R^d$, we wish to find a dictionary $\mathcal{D}\in\R^{d \times n}$ and a representation $\mathcal{R} = [r_1, \dots, r_k], \ r_i\in\R^n$ such that: $$\|\mathcal{X}-\mathcal{D}\mathcal{R}\|_F^2 \text{ is minimized and } \mathcal{R} \text{ is sparse enough.}$$

\noindent Depending on the aim of (a) finding a minimal representation for given precision or (b) finding a best approximation using up to $t$ variables the problem can be stated in a most general form as
\begin{alignat*}{5}
          \min_{\mathcal{D}, \mathcal{R}}  f(\mathcal{D}, \mathcal{R}; X) + g(\mathcal{D}) + h(\mathcal{R})      & \quad  & \\
  %        \text{s.t.} & \quad  &, & \quad \\
     %                 & \quad  &, & \quad 
\end{alignat*}
with $f$ representing the data-fidelity term (usually a measure for goodness of the approximation) and regularizers $g, h$ for the dictionary and representatives respectively.
\\
One way of stating the problem for (a) is
\begin{alignat}{5}
          \min_{\mathcal{D}, \mathcal{R}} \|\mathcal{X}-\mathcal{D}\mathcal{R}\|_F^2  + \lambda \sum_{i=1}^k  \|\ r_i\|_0    & \quad  & \\
         \text{s.t.} & \quad  \|d_j\|_2 \leq 1&, \forall j=1, ...,n & \quad 
\end{alignat}
with a regularizer on the amount of variables used and a further restriction on the dictionary vectors to avoid scaling problems. The $\| .\|_0$ is hereby defined as the number of non-zero entries making the whole problem computationally difficult to track\cite{NPHardproof}. A formulation for the case (b) is 
\begin{alignat}{5}
          \min_{\mathcal{R}} \sum_{i=1}^k  \|\ r_i\|_0    & \quad  & \\
         \text{s.t.} & \quad  \|\mathcal{X}-\mathcal{D}\mathcal{R}\|_F^2 \leq \epsilon & & \quad \\
          & \quad  \| r_i \|_0 \leq t  &, \forall i=1, ..., k & \quad 
\end{alignat}

%Which can be reclassified as an optimization problem: $$\argmin_{\mathcal{D}\in\mathcal{C}, r_i\in\R^n} \sum_{i=1}^k \|x_i - \mathcal{D}r_i\|_2^2 + \lambda\|r_i\|_0$$
%where $\mathcal{C} = \left\{\mathcal{D}\in\R^{d \times n} : \|d_i\|_2 \leq 1, \forall i=1, \dots, n\right\}$.

\noindent We abstract some of the functional definitions from the above statement for case (b) so as to represent the optimization problem in a more general form:
\begin{alignat*}{4}
    &\text{minimize }   & F(a) & \\
    &\text{subject to } & \text{support}(a)&\subseteq S\\
    &                   & |S|  &\leq t\\
\end{alignat*}
	

\subsection{Related Work}
In order to tackle the problem whose complexity mainly arises from the $\| .\|_0$ norm, one approach is to relax the norm to a more feasible one (i.e. LASSO, LARS for convex-relaxation or log penalty for non-convex relaxation). However, this might lead to over-penalization\cite{nonconvexrelax}.
\\
\\
Another ansatz is to use a greedy approach which constantly adds or removes variables based on some measure\cite{submod_spectral}.  This can be stated as a maximization problem of a submodular function. A submodular function thereby can be viewed as a discrete analogon to a convex function in the continuous case\cite{submod_sparsecoding}.
\\
\\
Applications of Sparse Dictionary Learning learn from classical Machine Learning (i.e. Linear Sparse Regression), Computer Vision(Image reconstruction, inpainting, denoising) to Signal Processing\cite{nonconvexrelax}\cite{submod_sparsecoding}.
%\textcolor{red}{A simple overview of prior work, highlighting the assumptions that have been made over time, especially the submodular function representation and resulting convex relaxation. This will lead to a discussion on greedy algorithms and the continuous greedy algorithm.}

\section{Data}
We are not yet aware of common data sets used in Sparse Regression/Dictionary Selection problems. As we are looking to develop a novel algorithm to provide a new benchmark for approximate solutions to this class of problems, we will likely assimilate the data and experiments used in  \cite{submod_spectral}, \cite{greedy_selection}, \cite{rIBP}, among others. 

\section{Deliverables} \label{methods}
As a result of this project, we anticipate that we will develop a novel greedy algorithm that operates on the approximation of supermodular functions within a convex relaxation of the Sparse Regression/Dictionary Selection problem. Implicit in the development of this algorithm will be several statements and definitions that recast the Sparse Regression/Dictionary Selection problem in terms of supermodular functions. In order to do so, we will need to develop a robust analog between Dictionary Selection and 2-stage sparse regression.
\\
\\
Our ultimate goal is to submit this work to the 2016 NIPS conference. The deadline for submissions is on 20 May 2016, which will provide us approximately 3 weeks after the final deadline for this course to clean up our presentation of our assumptions, methodology and model/algorithms.

\section{Next Steps} \label{next steps}
Our first step in this project will be to understand the supermodular minimzation framework in which sparse regression can be defined. Then, after having connected and identified sparse regression as supermodular minimization problem we want to show that dictionary selection is a 2-stage sparse regression. \\
\\
In the following step we need to understand the greedy algorithm for submodular maximization and its improvement, the continuous greedy algorithm. After this, understanding 2-stage submodular optimization would ideally allow us to connect the pieces, gain insight and finally propose a new algorithm of dealing with the problem.

%\textcolor{red}{What we've got to do!
%\begin{itemize}
%\item{Show sparse regression as supermodular minimization}
%\item{Show Dictionary Selection as 2-stage sparse regression}
%\item{Understand greedy algorithm of submodular maximization}
%\item{Understand the continuous greedy algorithm}
%\item{Read + understand 2-stage submodular optimization}
%\item{ATTACK!}
%\end{itemize}}
%
\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}